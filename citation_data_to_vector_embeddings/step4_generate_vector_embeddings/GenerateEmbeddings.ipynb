{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f800059-e141-4468-9372-41dbb34f8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/21 16:14:01 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_path = \"/home/shsa3327\"\n",
    "spark = SparkSession.builder\\\n",
    "    .config('spark.driver.memory', '40g')\\\n",
    "    .config('spark.executor.memory', '20g')\\\n",
    "    .config('spark.executor.cores', '30')\\\n",
    "    .config('spark.local.dir', f'{spark_path}/tmp') \\\n",
    "    .config('spark.driver.maxResultSize', '40g')\\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "    .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"1024\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config('spark.driver.extraJavaOptions', f'-Djava.io.tmpdir={spark_path}/tmp') \\\n",
    "    .config('spark.executor.extraJavaOptions', f'-Djava.io.tmpdir={spark_path}/tmp') \\\n",
    "    .config('hive.exec.scratchdir', f'{spark_path}/tmp/hive') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e5823d-b396-4ace-a2e7-3a84343b64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bf5ab6-e090-4f3d-baab-255236f815c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1501454"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_with_q_c_metadata = '/home/shsa3327/mypetalibrary/pmoa-cite-dataset/aggregated_dateset/combined_pubmed.parquet'\n",
    "combined_df_read = spark.read.parquet(combined_df_with_q_c_metadata)\n",
    "combined_df_read.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc4cccf-4660-4e17-8b12-9f28502dbd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pmid',\n",
       " 'secid',\n",
       " 'paraid',\n",
       " 'sentid',\n",
       " 'sentence',\n",
       " 'citations',\n",
       " 'relevance_score',\n",
       " 'q_pmid',\n",
       " 'q_doi',\n",
       " 'q_title',\n",
       " 'q_abstract',\n",
       " 'q_publication_year',\n",
       " 'q_cited_by_count',\n",
       " 'c_pmid',\n",
       " 'c_doi',\n",
       " 'c_title',\n",
       " 'c_abstract',\n",
       " 'c_publication_year',\n",
       " 'c_cited_by_count']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_read.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb6fb81-3c23-422f-8d33-7e5384a5ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from functools import lru_cache\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "@lru_cache(maxsize=None)\n",
    "def cosine_similarity(feature1, feature2):\n",
    "    # model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    feature1_embedding = model.encode(feature1)\n",
    "    feature2_embedding = model.encode(feature2)\n",
    "    cosine_similarity = util.cos_sim(feature1_embedding, feature2_embedding)\n",
    "    return cosine_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61fc36f-8758-454e-820a-16ec639662e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups: 4992\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "grouped_records = defaultdict(list)\n",
    "collected_data = combined_df_read.collect()\n",
    "for record in collected_data:\n",
    "    grouped_records[record['pmid']].append(record)\n",
    "print(f\"Total groups: {len(grouped_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e15e96-7aed-4a47-979f-d88b2c05bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▎                                                                                                                                                                                                                                                                                    | 23/4992 [06:51<24:41:32, 17.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m record\u001b[38;5;241m.\u001b[39mget(b):\n\u001b[1;32m     28\u001b[0m         record[b]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     embedding_arr\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m record\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_publication_year\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     32\u001b[0m     record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_publication_year\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(feature1, feature2)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_similarity\u001b[39m(feature1, feature2):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     feature1_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(feature1)\n\u001b[0;32m---> 12\u001b[0m     feature2_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     cosine_similarity \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(feature1_embedding, feature2_embedding)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/mpoa/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/mpoa/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mpoa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mpoa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mpoa/venv/lib/python3.10/site-packages/sentence_transformers/models/Pooling.py:85\u001b[0m, in \u001b[0;36mPooling.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     83\u001b[0m     output_vectors\u001b[38;5;241m.\u001b[39mappend(max_over_time)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode_mean_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode_mean_sqrt_len_tokens:\n\u001b[0;32m---> 85\u001b[0m     input_mask_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     sum_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(token_embeddings \u001b[38;5;241m*\u001b[39m input_mask_expanded, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m#If tokens are weighted (by WordWeights layer), feature 'token_weights_sum' will be present\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "embedded_groups = []\n",
    "cosine_pairs = [\n",
    "    ('q_title','c_title'),\n",
    "    ('q_title','c_abstract'),\n",
    "    ('q_abstract','c_title'),\n",
    "    ('q_abstract','c_abstract'),\n",
    "    ('sentence','c_abstract'),\n",
    "    ('sentence','c_title')\n",
    "]\n",
    "'''\n",
    "year_difference = q_year-c_year\n",
    "len_c_title = len(c_title)\n",
    "len_c_abstract = len(c_abstract)\n",
    "log_c_in_citations = np.log2(c_in_citations)\n",
    "'''\n",
    "start = time.perf_counter()\n",
    "for k,v in tqdm(grouped_records.items()):\n",
    "    for record in v:\n",
    "        embedding_arr = []\n",
    "        record = record.asDict()\n",
    "        for a,b in cosine_pairs:\n",
    "            if not record.get(a):\n",
    "                record[a]=''\n",
    "            if not record.get(b):\n",
    "                record[b]=''\n",
    "            embedding_arr.append(cosine_similarity(record.get(a,''),record.get(b,'')))\n",
    "\n",
    "        if not record.get('c_publication_year'):\n",
    "            record['c_publication_year']=0\n",
    "        if not record.get('q_publication_year'):\n",
    "            record['q_publication_year']=0    \n",
    "        embedding_arr.append(record.get('q_publication_year')-record.get('c_publication_year'))\n",
    "        embedding_arr.append(len(record.get('q_title','')))\n",
    "        embedding_arr.append(len(record.get('c_title','')))\n",
    "        embedding_arr.append(len(record.get('q_abstract','')))\n",
    "        embedding_arr.append(len(record.get('c_abstract','')))\n",
    "        if record.get('c_in_citations'):\n",
    "            embedding_arr.append(np.log2(record.get('c_in_citations')))\n",
    "        else:\n",
    "            embedding_arr.append(0)\n",
    "        embedding_arr.append(record.get('relevance_score'))\n",
    "        embedded_groups.append([k] + embedding_arr)\n",
    "end = time.perf_counter()\n",
    "print(f\"Time taken to create embeddings for {len(embedded_groups)} groups: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84975646-751c-4ce7-b641-f96c34d3a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = spark.createDataFrame(pd.DataFrame(embedded_groups))\n",
    "df.write.parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3a86-5bad-4bbd-a440-bf21ea591994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into test and train groups\n",
    "test_size = 0.2\n",
    "test_groups_size = int(len(embedded_groups)*test_size)\n",
    "train_groups_size = len(embedded_groups)-test_groups_size\n",
    "test_groups = list(embedded_groups.keys())[:test_groups_size]\n",
    "train_groups = list(embedded_groups.keys())[test_groups_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0c52c-a47b-4555-ae53-49886d0064e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "test_data =[]\n",
    "test_queries = []\n",
    "for test_group in test_groups:\n",
    "    for group in embedded_groups[test_group]:\n",
    "        test_data.append(group)\n",
    "        test_queries.append(test_group)\n",
    "X_test = [data[:-1] for data in test_data]\n",
    "y_test = [data[-1] for data in test_data]\n",
    "Counter(y_test).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57b623-f78d-4dce-9dce-a80075d90c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =[]\n",
    "train_queries = []\n",
    "for train_group in train_groups:\n",
    "    for group in embedded_groups[train_group]:\n",
    "        train_data.append(group)\n",
    "        train_queries.append(train_group)\n",
    "X_train = [data[:-1] for data in train_data]\n",
    "y_train = [data[-1] for data in train_data]\n",
    "Counter(y_train).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693c441-475c-490d-b194-e1a3f31ba229",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_relevance = max(np.max(y_train),np.max(y_test))\n",
    "y_train /= max_relevance\n",
    "y_test /= max_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da516294-b790-4eb4-a473-625f12345525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRanker, Pool, MetricVisualizer\n",
    "train = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_id=train_queries\n",
    ")\n",
    "\n",
    "test = Pool(\n",
    "    data=X_test,\n",
    "    label=y_test,\n",
    "    group_id=test_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec6fa7-b5c1-4833-92c8-60eb5f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = {\n",
    "    'iterations': 2000,\n",
    "    'custom_metric': ['NDCG', 'PFound', 'AverageGain:top=10'],\n",
    "    'verbose': False,\n",
    "    'random_seed': 0,\n",
    "}\n",
    "\n",
    "parameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109b858-13c3-48c7-8d6c-a8fe5cbc7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def fit_model(loss_function, additional_params=None, train_pool=train, test_pool=test):\n",
    "    parameters = deepcopy(default_parameters)\n",
    "    parameters['loss_function'] = loss_function\n",
    "    parameters['train_dir'] = loss_function\n",
    "\n",
    "    if additional_params is not None:\n",
    "        parameters.update(additional_params)\n",
    "\n",
    "    model = CatBoostRanker(**parameters)\n",
    "    model.fit(train_pool, eval_set=test_pool, plot=True)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
